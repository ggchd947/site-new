<!DOCTYPE html>
<html lang="en-us"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="Some Key Definitions

EA: Effective Altruism.
OPP: OPenPhil or Open Philanthropy, an organization that provides grants based on the doctrine of Effective Altruism.
x-risk: existential risk.
Peter Singer Theory: We should help people as long as we do not sacrifice anything &ldquo;morally significant&rdquo;.
Longtermism: Refers to a set of ethical views concerned with protecting and improving the long-run future.

Effective Altruism&rsquo;s Bait-and-Switch: From Global Poverty to AI Doomerism
The Effective Altruism founders planned - from day one - to mislead donors and new members in order to build the movement&rsquo;s brand and community:
">  

  <title>
    
      AI Doomerism
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.01273a70fa873b012d056499c16bb47955e0e7526c34edb73f05ca8f99f488ebc323423c6557f93f9b42a41de0448a25ce9a1ab577d0bf61e36eaf52a4979a1d.css" integrity="sha512-ASc6cPqHOwEtBWSZwWu0eVXg51JsNO23PwXKj5n0iOvDI0I8ZVf5P5tCpB3gRIolzpoatXfQv2Hjbq9SpJeaHQ==" />
  
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
<a href="/">..</a>


<article>
    <p class="post-meta">
        <time datetime="2024-05-02 17:50:29 -0700 PDT">
            2024-05-02
        </time>
    </p>

    <h1>AI Doomerism</h1>

    

    <h3 id="some-key-definitions">Some Key Definitions</h3>
<ul>
<li><strong>EA:</strong> Effective Altruism.</li>
<li><strong>OPP:</strong> <strong>OP</strong>en<strong>P</strong>hil or Open Philanthropy, an organization that provides grants based on the doctrine of Effective Altruism.</li>
<li><strong>x-risk:</strong> existential risk.</li>
<li><strong>Peter Singer Theory:</strong> We should help people as long as we do not sacrifice anything &ldquo;morally significant&rdquo;.</li>
<li><strong>Longtermism:</strong> Refers to a set of ethical views concerned with protecting and improving the long-run future.</li>
</ul>
<h3 id="effective-altruisms-bait-and-switch-from-global-poverty-to-ai-doomerism">Effective Altruism&rsquo;s Bait-and-Switch: From Global Poverty to AI Doomerism</h3>
<p>The Effective Altruism founders planned - from day one - to mislead donors and new members in order to build the movement&rsquo;s brand and community:
<img src="/img/ai-doomerism/mislead-donors.jpg" alt="mislead donors"></p>
<p>The public-facing discourse of &ldquo;giving to the poor&rdquo; was a mirage designed to get people into the Effective Altruism (EA) movement and then lead them to the &ldquo;core EA&rdquo;, x-risk, which is discussed in inward-facing spaces:
<img src="/img/ai-doomerism/giving-to-the-poor.jpeg" alt="giving to the poor"></p>
<blockquote>
<p>Effective Altruism founder, Will MacAskill - June 2012</p>
</blockquote>
<p><img src="/img/ai-doomerism/x-risk.jpg" alt="x-risk"></p>
<blockquote>
<p>Effective Altruism founder, Will MacAskill - November 2012</p>
</blockquote>
<p>The guidance was to promote the publicly-facing cause (global poverty) and keep quiet about the core cause (AI x-risk):
<img src="/img/ai-doomerism/global-poverty.jpg" alt="global poverty"></p>
<p>In 2014 &ndash; Peter Wildeford published a conversation about &ldquo;EA Marketing&rdquo; with EA communications specialist Michael Bitton.</p>
<p>Wildeford is the co-founder and co-CEO of Rethink Priorities and Chief Advisory Executive at IAPS (Institute for AI Policy and Strategy):
<img src="/img/ai-doomerism/iaps.png" alt="peter wildeford"></p>
<p>Jan Kulveit, who leads the European Summer Program on Rationality (ESPR), shared this on Facebook in 2018:
<img src="/img/ai-doomerism/espr.png" alt="Jan Kulveit"></p>
<p>As Effective Altruists engaged more deeply with the movement, they were encouraged to shift to AI x-risk:
<img src="/img/ai-doomerism/shift-to-ai.png" alt="shift to ai"></p>
<p>In 2019 &ndash; EA Hub published a guide: &ldquo;Tips to help your conversation go well.&rdquo;
Among the tips like &ldquo;Use the person&rsquo;s interest&rdquo;, there was &ldquo;Preventing &lsquo;Bait and Switch&rsquo;&rdquo;:
<img src="/img/ai-doomerism/bait-and-switch.png" alt="bait and switch"></p>
<h3 id="key-takeways">Key Takeways</h3>
<p>In the Public-facing/grassroots EAs (audience, followers, participants):</p>
<ol>
<li>The main focus is effective giving via Peter Singer Theory</li>
<li>The main cause area is global health, targeting the &lsquo;distant poor&rsquo; in developing countries.</li>
<li>The donors support organizations doing direct anti-poverty work.</li>
</ol>
<p>In the Core/highly engaged EAs (contributors, core, leadership):</p>
<ol>
<li>The main focus is x-risk/longtermism (Nick Bostrom &amp; Eliezer Yudkowsky).</li>
<li>The main cause areas are x-risk, AI-safety, &lsquo;global priorities research,&rsquo; and EA movement-building.</li>
<li>The donors support highly-engaged EAs to build career capital, boost their productivity, and/or start new EA organizations; research; policy-making/agenda setting.</li>
</ol>
<hr>

</article>

            </div>
        </main>
    </body></html>
